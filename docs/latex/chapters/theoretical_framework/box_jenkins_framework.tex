\chapter{Modelo de selección de Box-jenkins}
\label{chap:box_jenkins_framework}
Enders(año) se basa en la estrategia de \textit{Box-Jenkins} para hacer la selección del modelo más apropiado para series temporales unitarias de tipo \textit{ARIMA}, que se establece en tres etapas; identificación; estimación; diagnóstico. La etapa de \textit{identificación} se basa en herramientas visuales que permiten a simple vista detectar tendencias, quiebres estructurales, errores faltantes, datos atípicos, etc.
En la etapa de \textit{estimación} se seleccionan modelos estacionarios y parsimoniosos que se ajustan a los datos. Y por último, en el \textit{diagnóstico} se evalúa si los residuos se parecen en comportamiento a un ruido blanco. Se sigue este enfoque de tipo unitario y se lleva a un universo con más series temporales para el mismo rango de tiempo, armando un \textit{dataframe} que constituyen los datos de entrada para el modelo dinámico.

Algunas ideas claves relacionadas a Box-Jenkins son la parsimonia, estacionariedad e invertibilidad, bondad de ajuste y evaluación luego de la estimación.

\subsection{Concepto de parsimonia}
Con respecto a la parsimonia, implica que al incorporar más coeficientes en la estimación, se va a incrementar el ajuste  (el $R^2$ incrementa, por ejemplo) a costa de perder grados de libertad. Box-Jenkins sostiene que los modelos parsimoniosos performan mejor a la hora de realizar predicciones en comparación a modelos sobreparametrizados. Por este motivo, la parsimonia sería un objetivo a la hora de estimar modelos unitarios. Como en estudio, el foco no va a estar en realizar estimaciones unitarias, no se van a aplicar pruebas en los datos. Sin embargo, el próximo concepto si va a ser importante porque es un supuesto básico en los modelos \textit{VAR}, que son modelos fundamentales de los \textit{FAVAR}.




\subsection{Concepto de estacionariedad}
Se dice que un proceso estocástico $y_t$ con media y varianza finitas es estacionario en covarianza para todo $t$ y $t-s$ cuando

\begin{align}
\label{eq:2_7}
E(y_t)&=E(y_{t-s})=\mu\\
\label{eq:2_8}
var(y_t)&=var(y_{t-s})=\sigma^2_y\\
\label{eq:2_9}
cov(y_t,y_{t-s})&=cov(y_{t-j},y_{t-j-s})=\gamma_s
\end{align}
donde $\mu, \sigma^2_y, \gamma_s$ con constantes.
   
En~\eqref{eq:2_8}, cuando $s=0$ se va a tener que $\gamma_0$ es la varianza de $y_t$. Que una serie temporal sea estacionaria en covarianza implica que tanto la media como todas sus autocovarianzas\footnote{También se habla de proceso debilmente estacionario o estacionario de segundo orden.} no están afectadas por cambios en los orígenes temporales. En modelos multivariados, como es el caso en este estudio, el término autocovarianza refiere a la covarianza entre $y_t$ y sus propios rezagos mientras que covarianza cruzada refiere a la covarianza entre series temporales.

Para series estacionarias de segundo orden, se define la autocorrelación de $y_t$ y $y_{t-s}$ como

\begin{equation*}
\rho_s\equiv\gamma_s/\gamma_0
\end{equation*} 

Como $\gamma_s/$ y $\gamma_0$ son independientes del tiempo, entonces los coeficientes de correlación $\rho_s$ también van a ser independientes del tiempo. Si bien la autocorrelación entre $y_t$ y $y_{t-1}$ podría ser distinta a la autocorrelación entre $y_t$ y $y_{t-2}$, la autocorrelación entre $y_t$ y $y_{t-1}$ va a ser igual a la autocorrelación entre $y_t$ y $y_{t-s-1}$. Asimismo, $\rho_0=1$.

Las autocovarianzas y las autocorrelaciones son herramientas esenciales en la metodología de Box-Jenkins(1976) para identificar y estimar los modelos de series temporales. 


Como que se está en un marco de ecuaciones en diferencias, se tienen que cumplir condiciones de estabilidad en el sistema y esto implica que las raíces características asociadas al sistema de interés caígan dentro del círculo unitario. De modo que para cada serie temporal estacionaria, individualmente considerada, su correlograma, que consiste en graficar $\rho_s$ contra $s$ debe converger a cero.  Además, cuando se quiera eliminar el efecto de los valores intermedios $y_{t-1}, ..., y_{t-s+1}$  entre  $y_t$ y $y_{t-s}$ se puede emplear la autocorrelación parcial dado que el correlograma incluye las correlaciones indirectas del proceso autorregresivo. Las autocorrelaciones parciales se calculan como

\begin{align}
\label{eq: 2_35}
\phi_{11}&=\rho_1\\
\label{eq: 2_36}
\phi_{22}&=(\rho_2-\rho_1^2)/(1-\rho_1^2)\\
\nonumber
\text{para rezagos adicionales,}\\
\label{eq: 2_37}
\phi_{ss}&=\frac{\rho_s-\sum_{j=1}^{s-1}\phi_{s-1,j}\rho_{s-j}}{a-\sum_{j=1}^{s-1}\phi_{s-1,j}\rho_j}
\end{align}
donde $\phi_{sj}=\phi_{s-1,j}-\phi_{ss}\phi_{s-1,s-j}$ con $j=1,2,3,...,s-1$.

En la etapa de identificación, se analizan los gráficos de cada una de las series univariadas, sus correlogramas y las funciones de correlación parcial. Esto permite detectar datos atípicos, faltantes y cambios estructurales en las mismas. Las series no estacionarias, podrían presentar una evidente tendencia, medias y varianzas que no son constantes en el tiempo.

Una concepto fundamental de este enfoque es el de parsimonia. Implica que al adicionar coeficientes al modelo se aumenta el ajuste (el valor de $R^2$) a costa de la reducción de los grados de libertad del modelo. Box y Jenkins señalan que los modelos parsimoniosos generan mejoren predicciones que aquellos modelos sobreparametrizados. En este contexto se sugiere emplear tanto los criterios AIC como SBC como medidas apropiadas del ajuste general del modelo.

Otro concepto clave en este enforque es el de la invertibilidad. Se dice que $y_t$ es invertible si puede ser representada por un proceso autorregresivo convergente o de primer order.

Un supuesto fundamental del enfoque Box-Jenkins es que la estructura del proceso generativo de los datos no presenta cambios. Para evaluar esto para cada serie temporal se puede emplear la prueba de Chow. Sin embargo, el test de Chow y sus variantes asumen que el cambio se manifiesta en los datos, es decir, es evidente. Entre varias técnicas que se desarrollaron en la literatura, se podría usar la propuesta por Brown, Durbin, Evans (1975) que consiste en hallar la suma acumulada de los errores de predicción para cada serie y evaluar si es distinta de cero:

\begin{equation*}
CUSUM_N=\sum_{i=n}^{N}e_i(1)/\sigma_e,\quad N=n,...,T-1    
\end{equation*}
donde $n$ es la fecha para el primer error de predicción y $T$ la última observación del conjunto de datos y $\sigma_e$ la desviación estándar estimada de los errores de predicción.
