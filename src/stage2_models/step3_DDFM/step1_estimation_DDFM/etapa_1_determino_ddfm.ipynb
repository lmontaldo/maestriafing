{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "En la presente etapa del estudio se busca determinar los hiperparámetros del modelo. Una vez que se tenga aquella combinación con menor pérdida, se pasa a estimar el modelo en una segunda etapa."
      ],
      "metadata": {
        "id": "Cd4nMBK48QS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys,inspect\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "parentdir = os.path.dirname(currentdir)\n",
        "parentdir = os.path.dirname(parentdir)\n",
        "sys.path.insert(0,parentdir)"
      ],
      "metadata": {
        "id": "IUTkX92sKF5N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xll83j-0QPuY",
        "outputId": "9a6bda02-b310-4459-da7c-179fb1c75f3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/D2FM')\n",
        "module_path = '/content/drive/My Drive/D2FM'\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "metadata": {
        "id": "OHQp36zkQZ_o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykalman"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS3rYn20WLyj",
        "outputId": "c0c645f5-e2ca-4cd5-b4e0-7fb56d49ff0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykalman\n",
            "  Downloading pykalman-0.9.5.tar.gz (228 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/228.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/228.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pykalman\n",
            "  Building wheel for pykalman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykalman: filename=pykalman-0.9.5-py3-none-any.whl size=48442 sha256=fa30d4abb1df13ca5ab3482f1d7111bc0797e33ed3277e6b3adc469b756b705d\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/33/ef/5f332226e13a5089c6dd4b01cc2bcb59491d18f955fa2d3807\n",
            "Successfully built pykalman\n",
            "Installing collected packages: pykalman\n",
            "Successfully installed pykalman-0.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.ddfm import *"
      ],
      "metadata": {
        "id": "sfZM6nNNVDp0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "--U2KPzqary4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from statsmodels.tsa.statespace.dynamic_factor_mq import DynamicFactorMQ as DFM\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cgoKGqbd-ea"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jk9uMGiReqjf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from typing import Tuple\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s5TS6VMnrDrw"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/D2FM/scaled_train.csv')\n",
        "df_test=pd.read_csv('/content/drive/MyDrive/D2FM/scaled_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_idx=df_train.set_index(\"Index\")\n",
        "df_test_idx=df_test.set_index(\"Index\")"
      ],
      "metadata": {
        "id": "0-Kz_xZDgJ5z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K=1 para determinar los hiperparámetros del modelo. Son 64 combinaciones en una sola validación. Se determina el \"mejor\" conjunto de hiperparámetros en base a la pérdida menor."
      ],
      "metadata": {
        "id": "ZSNwENQ1XKii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "# Your existing code\n",
        "r = 6\n",
        "structure_encoders = [(r * 6, r * 4, r * 2, r),\n",
        "                      (r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r), (r * 48, r * 24, r * 16, r)]\n",
        "\n",
        "lags_inputs = list(range(2))\n",
        "link_options = ['tanh', 'relu']  # New link options\n",
        "batch_size_options = [24, 100]   # New batch size options\n",
        "\n",
        "model_info = []\n",
        "\n",
        "# Iterate over both True and False values for use_bias\n",
        "for use_bias_setting in [True, False]:\n",
        "    # Iterate over each structure_encoder configuration\n",
        "    for structure_encoder in structure_encoders:\n",
        "        # Iterate over each lag value\n",
        "        for lags_input in lags_inputs:\n",
        "            # Iterate over each link option\n",
        "            for link_option in link_options:\n",
        "                # Iterate over each batch size option\n",
        "                for batch_size_option in batch_size_options:\n",
        "                    # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                    ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                    # Start the timer for each individual model\n",
        "                    start_time_model = time.time()\n",
        "\n",
        "                    # Fit the model\n",
        "                    ddfm.fit()\n",
        "\n",
        "                    # Stop the timer for each individual model\n",
        "                    end_time_model = time.time()\n",
        "\n",
        "                    # Calculate the execution time for each individual model\n",
        "                    execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                    # Store the model, its configuration, final loss, and execution time\n",
        "                    final_loss = ddfm.loss_now\n",
        "                    model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                                       execution_time_model, link_option, batch_size_option))\n",
        "\n",
        "# Calculate the total execution time\n",
        "end_time_total = time.time()\n",
        "total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "# Print the information for each model\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option in model_info:\n",
        "    print(f\"Model with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option} - Final Loss: {final_loss}\")\n",
        "    print(f\"Execution Time for this model: {execution_time_model:.2f} seconds\")\n",
        "\n",
        "print(f\"Total Execution Time for all models: {total_execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrKEnejHJmD",
        "outputId": "945740cb-c41b-40fc-d17d-6db131ad2626"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6062719455379042 - delta: 0.0001527789508580558 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6042899097289977 - delta: 0.0003065221216681004 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6023843152966347 - delta: 3.685885736727355e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.598491798566092 - delta: 0.00015911664960780816 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6002428834341017 - delta: 0.00022240091485573735 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5910242783356396 - delta: 0.00034407843139337653 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5956880450291226 - delta: 0.0003653389912184356 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5914107026509534 - delta: 0.00017277716381314826 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6049391147003528 - delta: 4.43864744965334e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5967772255100906 - delta: 4.5609778047947735e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6029460508896672 - delta: 0.00044028349341887143 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5954505126447965 - delta: 7.832076467386196e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5964943410600487 - delta: 0.00018666113470886596 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5899213905144091 - delta: 0.0004646270748447459 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5949323333126293 - delta: 0.0001925431542131662 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5912581728510942 - delta: 0.0003789040012973617 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6003987766262829 - delta: 0.0001032932313131454 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.593671649027021 - delta: 0.00032187867059967496 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6040446630202766 - delta: 0.000215595811448479 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5942885950769177 - delta: 0.00038332508857920637 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5935188389604059 - delta: 0.0004870004825115737 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897715899469995 - delta: 0.00027790505958715824 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5969073764109344 - delta: 0.00023307122122015749 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5911522079270886 - delta: 0.00030634811276733097 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5981758686034958 - delta: 0.0004829542089740189 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5921660183819364 - delta: 0.0003355168475193326 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6070911675236714 - delta: 0.000300076734324492 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5960229812381836 - delta: 0.00020664514928323208 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5927569656701296 - delta: 0.00046259972431036966 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.58967100937883 - delta: 0.0004644089524920016 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5979895264273086 - delta: 0.00010401898039127872 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5907699456273472 - delta: 8.132066577197144e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6062548934065508 - delta: 8.275697284395381e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.602727405145712 - delta: 6.244500355134345e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6308891588908048 - delta: 0.00017148620474028797 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6181232528313846 - delta: 0.00027063700397867297 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5984879606530491 - delta: 0.00030098281833463263 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5921400058562918 - delta: 0.00039660292308030015 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6219090376198234 - delta: 0.00038408998008114526 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6090994685171214 - delta: 0.0004812029318313043 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6053986780251778 - delta: 0.00039232346802667295 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.596581662720132 - delta: 0.0004589913492979657 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6295127707356876 - delta: 0.00044012391129523746 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.614537528427105 - delta: 0.0004365875473530747 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5960004367749229 - delta: 0.0004874662874916214 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5906107643512373 - delta: 0.00042970398916054923 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6183733277886364 - delta: 0.00013178379302344008 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6082655107916747 - delta: 0.0003275360061450031 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5984035437947037 - delta: 0.00018014127683493717 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5951782881046199 - delta: 3.425342455849549e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6327597742232652 - delta: 0.00016205049165547844 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5936419537010068 - delta: 0.00046950706780443305 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5900544032161384 - delta: 0.0004973231190222807 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.615847745675623 - delta: 3.4310620136067316e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6076975918395733 - delta: 0.0004126154247404874 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5968888891013979 - delta: 0.00037840389436479915 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.592358968378782 - delta: 0.0003966710950347309 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6291393594869089 - delta: 0.00019733083599424208 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6097625812669517 - delta: 0.00024543045270425466 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5930709867386083 - delta: 0.00045146161667056667 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897144527534139 - delta: 0.00044150957760852127 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6218347326856096 - delta: 0.0002936154484380311 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6091593557346485 - delta: 0.0003080609697254865 < 0.0005\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6062719455379042\n",
            "Execution Time for this model: 106.63 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.6042899097289977\n",
            "Execution Time for this model: 77.59 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6023843152966347\n",
            "Execution Time for this model: 64.69 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.598491798566092\n",
            "Execution Time for this model: 48.78 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6002428834341017\n",
            "Execution Time for this model: 89.64 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5910242783356396\n",
            "Execution Time for this model: 124.51 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5956880450291226\n",
            "Execution Time for this model: 77.37 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5914107026509534\n",
            "Execution Time for this model: 79.49 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6049391147003528\n",
            "Execution Time for this model: 100.84 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5967772255100906\n",
            "Execution Time for this model: 130.39 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6029460508896672\n",
            "Execution Time for this model: 78.77 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5954505126447965\n",
            "Execution Time for this model: 90.23 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5964943410600487\n",
            "Execution Time for this model: 87.74 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5899213905144091\n",
            "Execution Time for this model: 103.29 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5949323333126293\n",
            "Execution Time for this model: 77.84 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5912581728510942\n",
            "Execution Time for this model: 78.39 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6003987766262829\n",
            "Execution Time for this model: 79.24 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.593671649027021\n",
            "Execution Time for this model: 103.35 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6040446630202766\n",
            "Execution Time for this model: 65.67 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5942885950769177\n",
            "Execution Time for this model: 68.47 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5935188389604059\n",
            "Execution Time for this model: 73.87 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5897715899469995\n",
            "Execution Time for this model: 98.33 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5969073764109344\n",
            "Execution Time for this model: 59.52 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5911522079270886\n",
            "Execution Time for this model: 92.67 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5981758686034958\n",
            "Execution Time for this model: 89.20 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5921660183819364\n",
            "Execution Time for this model: 120.82 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6070911675236714\n",
            "Execution Time for this model: 89.55 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5960229812381836\n",
            "Execution Time for this model: 51.70 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5927569656701296\n",
            "Execution Time for this model: 93.99 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.58967100937883\n",
            "Execution Time for this model: 92.57 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5979895264273086\n",
            "Execution Time for this model: 78.69 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5907699456273472\n",
            "Execution Time for this model: 78.98 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.6062548934065508\n",
            "Execution Time for this model: 75.34 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.602727405145712\n",
            "Execution Time for this model: 86.65 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6308891588908048\n",
            "Execution Time for this model: 120.70 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6181232528313846\n",
            "Execution Time for this model: 100.17 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5984879606530491\n",
            "Execution Time for this model: 86.07 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5921400058562918\n",
            "Execution Time for this model: 86.51 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6219090376198234\n",
            "Execution Time for this model: 60.16 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6090994685171214\n",
            "Execution Time for this model: 103.41 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.6053986780251778\n",
            "Execution Time for this model: 61.55 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.596581662720132\n",
            "Execution Time for this model: 110.28 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6295127707356876\n",
            "Execution Time for this model: 60.67 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.614537528427105\n",
            "Execution Time for this model: 111.85 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5960004367749229\n",
            "Execution Time for this model: 75.65 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5906107643512373\n",
            "Execution Time for this model: 81.79 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6183733277886364\n",
            "Execution Time for this model: 72.95 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6082655107916747\n",
            "Execution Time for this model: 76.76 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5984035437947037\n",
            "Execution Time for this model: 77.20 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5951782881046199\n",
            "Execution Time for this model: 107.97 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6327597742232652\n",
            "Execution Time for this model: 77.64 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6111559978328007\n",
            "Execution Time for this model: 133.03 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5936419537010068\n",
            "Execution Time for this model: 74.19 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5900544032161384\n",
            "Execution Time for this model: 92.08 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.615847745675623\n",
            "Execution Time for this model: 94.08 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6076975918395733\n",
            "Execution Time for this model: 95.74 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5968888891013979\n",
            "Execution Time for this model: 91.24 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.592358968378782\n",
            "Execution Time for this model: 103.44 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6291393594869089\n",
            "Execution Time for this model: 67.05 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6097625812669517\n",
            "Execution Time for this model: 132.32 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5930709867386083\n",
            "Execution Time for this model: 93.81 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5897144527534139\n",
            "Execution Time for this model: 104.88 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6218347326856096\n",
            "Execution Time for this model: 76.57 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6091593557346485\n",
            "Execution Time for this model: 77.63 seconds\n",
            "Total Execution Time for all models: 5623.29 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option = min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1zf-3RHeHj",
        "outputId": "d5889c7a-4940-456d-9a76-caf87a2f8491"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Loss: 0.58967100937883 with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para menos combinaciones de hiperparámetros, queda como ejemplo:"
      ],
      "metadata": {
        "id": "8TrKdbK9YDgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "# Your existing code\n",
        "r = 6\n",
        "structure_encoders = [(r * 6, r * 4, r * 2, r),\n",
        "                      (r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r), (r * 48, r * 24, r * 16, r),]\n",
        "\n",
        "lags_inputs = list(range(2))\n",
        "\n",
        "model_info = []\n",
        "\n",
        "# Iterate over both True and False values for use_bias\n",
        "for use_bias_setting in [True, False]:\n",
        "    # Iterate over each structure_encoder configuration\n",
        "    for structure_encoder in structure_encoders:\n",
        "        # Iterate over each lag value\n",
        "        for lags_input in lags_inputs:\n",
        "            # Initialize the DDFM with the current configuration and use_bias setting\n",
        "            ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                        optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link='tanh',\n",
        "                        epochs=100, max_iter=1000)\n",
        "\n",
        "            # Start the timer for each individual model\n",
        "            start_time_model = time.time()\n",
        "\n",
        "            # Fit the model\n",
        "            ddfm.fit()\n",
        "\n",
        "            # Stop the timer for each individual model\n",
        "            end_time_model = time.time()\n",
        "\n",
        "            # Calculate the execution time for each individual model\n",
        "            execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "            # Store the model, its configuration, final loss, and execution time\n",
        "            final_loss = ddfm.loss_now\n",
        "            model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss, execution_time_model))\n",
        "\n",
        "# Calculate the total execution time\n",
        "end_time_total = time.time()\n",
        "total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "# Print the information for each model\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model in model_info:\n",
        "    print(f\"Model with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting} - Final Loss: {final_loss}\")\n",
        "    print(f\"Execution Time for this model: {execution_time_model:.2f} seconds\")\n",
        "\n",
        "print(f\"Total Execution Time for all models: {total_execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCmpQtbS8wLt",
        "outputId": "0f97f50d-9ae9-4bd9-bfd7-e073b05c607d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6000213739457754 - delta: 2.948570757007814e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5981605811324008 - delta: 0.00041992750411991146 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6002641262117545 - delta: 0.0003938048813976006 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5901988115578353 - delta: 0.00020263298102667096 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5949236383805527 - delta: 3.758155916742125e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5899904092776949 - delta: 0.0004966915807349267 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5920543169590102 - delta: 0.00045281531054624754 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897366841627117 - delta: 2.1335932081187726e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6018477178373662 - delta: 0.0002652733344047113 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5918873194336332 - delta: 0.0004368461688436822 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5986105833380825 - delta: 0.00018390257867544776 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5900989855087982 - delta: 0.0004843253657673757 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5953852975705818 - delta: 0.0003933090511046878 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5907555267971585 - delta: 0.0004861394215376436 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5922850032041527 - delta: 0.0004212111463192132 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5896470056018115 - delta: 0.00037551778942354077 < 0.0005\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True - Final Loss: 0.6000213739457754\n",
            "Execution Time for this model: 95.43 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True - Final Loss: 0.5981605811324008\n",
            "Execution Time for this model: 48.35 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True - Final Loss: 0.6002641262117545\n",
            "Execution Time for this model: 70.12 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True - Final Loss: 0.5901988115578353\n",
            "Execution Time for this model: 92.47 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True - Final Loss: 0.5949236383805527\n",
            "Execution Time for this model: 105.76 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True - Final Loss: 0.5899904092776949\n",
            "Execution Time for this model: 83.75 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True - Final Loss: 0.5920543169590102\n",
            "Execution Time for this model: 118.98 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True - Final Loss: 0.5897366841627117\n",
            "Execution Time for this model: 108.83 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False - Final Loss: 0.6018477178373662\n",
            "Execution Time for this model: 68.66 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False - Final Loss: 0.5918873194336332\n",
            "Execution Time for this model: 100.38 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False - Final Loss: 0.5986105833380825\n",
            "Execution Time for this model: 76.38 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False - Final Loss: 0.5900989855087982\n",
            "Execution Time for this model: 90.85 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False - Final Loss: 0.5953852975705818\n",
            "Execution Time for this model: 70.70 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False - Final Loss: 0.5907555267971585\n",
            "Execution Time for this model: 93.51 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False - Final Loss: 0.5922850032041527\n",
            "Execution Time for this model: 107.06 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False - Final Loss: 0.5896470056018115\n",
            "Execution Time for this model: 107.68 seconds\n",
            "Total Execution Time for all models: 1439.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddfm6 = DDFM(df_train_idx, structure_encoder=(288, 144, 96, 6), lags_input=1, factor_oder=1,\n",
        "                             use_bias=False, link='tanh', max_iter=1000)\n",
        "ddfm6.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWbuZwx_6MV",
        "outputId": "9f89ed79-9f4e-4827-ee9c-11d5ca3bc0bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.589643687159019 - delta: 0.00012052289014438879 < 0.0005\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}