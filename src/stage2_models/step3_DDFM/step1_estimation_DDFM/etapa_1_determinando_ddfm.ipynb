{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "En la presente etapa del estudio se busca determinar los hiperparámetros del modelo. Una vez que se tenga aquella combinación con menor pérdida, se pasa a estimar el modelo en una segunda etapa."
      ],
      "metadata": {
        "id": "Cd4nMBK48QS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os,sys,inspect\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "parentdir = os.path.dirname(currentdir)\n",
        "parentdir = os.path.dirname(parentdir)\n",
        "sys.path.insert(0,parentdir)"
      ],
      "metadata": {
        "id": "IUTkX92sKF5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xll83j-0QPuY",
        "outputId": "2a536aa0-c470-434c-cca2-62b351443628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/D2FM')\n",
        "module_path = '/content/drive/My Drive/D2FM'\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ],
      "metadata": {
        "id": "OHQp36zkQZ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykalman"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS3rYn20WLyj",
        "outputId": "9ed4b601-4384-45e3-ce78-12695e07cd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pykalman\n",
            "  Downloading pykalman-0.9.7-py2.py3-none-any.whl (251 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/251.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/251.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pykalman) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pykalman) (1.11.4)\n",
            "Installing collected packages: pykalman\n",
            "Successfully installed pykalman-0.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.ddfm import *"
      ],
      "metadata": {
        "id": "sfZM6nNNVDp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--U2KPzqary4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from statsmodels.tsa.statespace.dynamic_factor_mq import DynamicFactorMQ as DFM\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cgoKGqbd-ea"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk9uMGiReqjf"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from typing import Tuple\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5TS6VMnrDrw"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/D2FM/scaled_train.csv')\n",
        "df_test=pd.read_csv('/content/drive/MyDrive/D2FM/scaled_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_idx=df_train.set_index(\"Index\")\n",
        "df_test_idx=df_test.set_index(\"Index\")"
      ],
      "metadata": {
        "id": "0-Kz_xZDgJ5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K=1 para determinar los hiperparámetros del modelo. Son X combinaciones en una sola validación. Se determina el \"mejor\" conjunto de hiperparámetros en base a la pérdida menor."
      ],
      "metadata": {
        "id": "ZSNwENQ1XKii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outer loop for r values\n",
        "for r in [5]:\n",
        "    start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "    structure_encoders = [(r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r)]\n",
        "\n",
        "    lags_inputs = list(range(2))\n",
        "    link_options = ['tanh', 'relu']\n",
        "    batch_size_options = [72, 100]\n",
        "\n",
        "    model_info = []\n",
        "\n",
        "    # Iterate over both True and False values for use_bias\n",
        "    for use_bias_setting in [True, False]:\n",
        "        # Iterate over each structure_encoder configuration\n",
        "        for structure_encoder in structure_encoders:\n",
        "            # Iterate over each lag value\n",
        "            for lags_input in lags_inputs:\n",
        "                # Iterate over each link option\n",
        "                for link_option in link_options:\n",
        "                    # Iterate over each batch size option\n",
        "                    for batch_size_option in batch_size_options:\n",
        "                        # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                        ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                    optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                    batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                        # Start the timer for each individual model\n",
        "                        start_time_model = time.time()\n",
        "\n",
        "                        # Fit the model\n",
        "                        ddfm.fit()\n",
        "\n",
        "                        # Stop the timer for each individual model\n",
        "                        end_time_model = time.time()\n",
        "\n",
        "                        # Calculate the execution time for each individual model\n",
        "                        execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                        # Store the model, its configuration, final loss, and execution time\n",
        "                        final_loss = ddfm.loss_now\n",
        "                        # Store the model, its configuration, final loss, execution time, and r value\n",
        "                        model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                              execution_time_model, link_option, batch_size_option, r))\n",
        "\n",
        "\n",
        "    # Calculate the total execution time\n",
        "    end_time_total = time.time()\n",
        "    total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    model_info_dict_list = []\n",
        "    for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option , r_value in model_info:\n",
        "        model_info_dict = {\n",
        "            \"encoder_config\": encoder_config,\n",
        "            \"lag\": lag,\n",
        "            \"use_bias_setting\": use_bias_setting,\n",
        "            \"final_loss\": final_loss,\n",
        "            \"execution_time_model\": execution_time_model,\n",
        "            \"link_option\": link_option,\n",
        "            \"batch_size_option\": batch_size_option,\n",
        "            \"r_value\": r_value\n",
        "        }\n",
        "        model_info_dict_list.append(model_info_dict)\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    with open(f\"/content/drive/MyDrive/D2FM/model_info_r5.json\", \"w\") as json_file:\n",
        "        json.dump(model_info_dict_list, json_file)\n",
        "\n",
        "    print(f\"Results for r={r} saved to model_info_r{r}.json\")\n",
        "    print(f\"Total Execution Time for all models for r={r}: {total_execution_time:.2f} seconds\")\n",
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option, r_value in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, r_value)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option , r_value= min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}, r {r_value}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spGAcQLir9PJ",
        "outputId": "1e3a8d00-2f49-4d53-c9e3-420a3e28561c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6337849734197494 - delta: 0.00012342317393720145 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6299177772556381 - delta: 0.00011907978845490698 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.644579046094442 - delta: 0.00034606306938838287 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6285120588164306 - delta: 0.0002369827365331642 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6224360048604499 - delta: 0.0003318787530264389 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6222124204235917 - delta: 0.0002746041294593474 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6219251546406112 - delta: 0.00036320805143707184 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6227024450862295 - delta: 0.0001436496344869238 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6350044294801358 - delta: 0.0004466211784320022 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6262254385669549 - delta: 0.00021040072704206324 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6262248941578651 - delta: 0.00015740489110743528 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6220077206175504 - delta: 0.0004029814398766541 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6217263388852007 - delta: 0.0004375792356646798 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.622642988684315 - delta: 0.0001684528994615042 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6227583998669847 - delta: 0.00017481984824037315 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6360112885796042 - delta: 4.224807378758138e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6295915269799608 - delta: 0.00045360566911437604 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.666857777538221 - delta: 0.00023854494044195 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.650650841382277 - delta: 0.00012599619680379761 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6226320630211449 - delta: 0.00047129920647796445 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6217616913839512 - delta: 0.00040406555697731665 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.646009235746164 - delta: 0.00045393870760392676 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6448475140822819 - delta: 0.0004532514161450891 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6327032097822695 - delta: 1.998684112074407e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.625730208028446 - delta: 0.00016827526792626072 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6647730667554039 - delta: 0.000238596861074994 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6484623880714512 - delta: 0.00044327810256594095 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6215284234293389 - delta: 1.68952127968056e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.621656277223813 - delta: 0.000397519754140963 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.64751784937184 - delta: 9.941077693129005e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6457124476104116 - delta: 0.0003468862517545221 < 0.0005\n",
            "Results for r=5 saved to model_info_r5.json\n",
            "Total Execution Time for all models for r=5: 4023.40 seconds\n",
            "Minimum Loss: 0.6215284234293389 with encoder (120, 60, 40, 5), lag 1, use_bias False, link tanh, batch_size 72, r 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Outer loop for r values\n",
        "for r in [6]:\n",
        "    start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "    structure_encoders = [(r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r)]\n",
        "\n",
        "    lags_inputs = list(range(2))\n",
        "    link_options = ['tanh', 'relu']\n",
        "    batch_size_options = [72, 100]\n",
        "\n",
        "    model_info = []\n",
        "\n",
        "    # Iterate over both True and False values for use_bias\n",
        "    for use_bias_setting in [True, False]:\n",
        "        # Iterate over each structure_encoder configuration\n",
        "        for structure_encoder in structure_encoders:\n",
        "            # Iterate over each lag value\n",
        "            for lags_input in lags_inputs:\n",
        "                # Iterate over each link option\n",
        "                for link_option in link_options:\n",
        "                    # Iterate over each batch size option\n",
        "                    for batch_size_option in batch_size_options:\n",
        "                        # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                        ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                    optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                    batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                        # Start the timer for each individual model\n",
        "                        start_time_model = time.time()\n",
        "\n",
        "                        # Fit the model\n",
        "                        ddfm.fit()\n",
        "\n",
        "                        # Stop the timer for each individual model\n",
        "                        end_time_model = time.time()\n",
        "\n",
        "                        # Calculate the execution time for each individual model\n",
        "                        execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                        # Store the model, its configuration, final loss, and execution time\n",
        "                        final_loss = ddfm.loss_now\n",
        "                        # Store the model, its configuration, final loss, execution time, and r value\n",
        "                        model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                              execution_time_model, link_option, batch_size_option, r))\n",
        "\n",
        "\n",
        "    # Calculate the total execution time\n",
        "    end_time_total = time.time()\n",
        "    total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    model_info_dict_list = []\n",
        "    for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option , r_value in model_info:\n",
        "        model_info_dict = {\n",
        "            \"encoder_config\": encoder_config,\n",
        "            \"lag\": lag,\n",
        "            \"use_bias_setting\": use_bias_setting,\n",
        "            \"final_loss\": final_loss,\n",
        "            \"execution_time_model\": execution_time_model,\n",
        "            \"link_option\": link_option,\n",
        "            \"batch_size_option\": batch_size_option,\n",
        "            \"r_value\": r_value\n",
        "        }\n",
        "        model_info_dict_list.append(model_info_dict)\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    with open(f\"/content/drive/MyDrive/D2FM/model_info_r5.json\", \"w\") as json_file:\n",
        "        json.dump(model_info_dict_list, json_file)\n",
        "\n",
        "    print(f\"Results for r={r} saved to model_info_r{r}.json\")\n",
        "    print(f\"Total Execution Time for all models for r={r}: {total_execution_time:.2f} seconds\")\n",
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option, r_value in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, r_value)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option , r_value= min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}, r {r_value}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1bvwZb-w2Ok",
        "outputId": "415d96d2-03db-4650-a4b6-b8357a0b87e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.6034720033485297 - delta: 0.00023677804003644545 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5972880867586865 - delta: 0.00046984519310047976 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6055026109538978 - delta: 6.382738273784761e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.598249991370626 - delta: 1.1014098662169128e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.590212278963928 - delta: 0.00046660506371375295 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5900214361188332 - delta: 0.00025494936331019186 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5903205877442683 - delta: 0.0003722397882741624 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5909891855493171 - delta: 0.0003592252474759947 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6049153507065753 - delta: 0.0003150795814652369 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5949647915368521 - delta: 0.0004343096353096656 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6051664405490365 - delta: 0.00014845079849910674 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5933790025828928 - delta: 0.00019623860699257477 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5899899897588007 - delta: 0.0004591497004051146 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.589956869321811 - delta: 0.00031871871855664464 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5903605920246568 - delta: 0.00018822649276611908 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5901818217551233 - delta: 0.0003116593557711951 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6024775493016921 - delta: 0.0001281548175201841 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5960284471695939 - delta: 0.0003296378540642649 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6314610965700793 - delta: 0.00027261080669207046 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.6143935110976008 - delta: 0.00044419069681582923 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5903456049992766 - delta: 0.0004492989940389201 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5901555611356222 - delta: 0.0003354293249139584 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6083457694265041 - delta: 0.00047375988850193037 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6076667745273717 - delta: 0.0002931300070830543 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 5ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6037216585417345 - delta: 0.00021306396416692806 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5932124085287889 - delta: 0.0001664724151955243 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6295539326238803 - delta: 0.00014857329269858996 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 6ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6119090468302112 - delta: 0.00048714614007552983 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5903791700345065 - delta: 0.00046628700782808255 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5900309558642297 - delta: 0.00037674579982032575 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6075323876606966 - delta: 0.000420141436328255 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6076779219354796 - delta: 0.00041481814125414685 < 0.0005\n",
            "Results for r=6 saved to model_info_r6.json\n",
            "Total Execution Time for all models for r=6: 4426.96 seconds\n",
            "Minimum Loss: 0.589956869321811 with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 100, r 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Outer loop for r values\n",
        "for r in [7]:\n",
        "    start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "    structure_encoders = [(r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r)]\n",
        "\n",
        "    lags_inputs = list(range(2))\n",
        "    link_options = ['tanh', 'relu']\n",
        "    batch_size_options = [72, 100]\n",
        "\n",
        "    model_info = []\n",
        "\n",
        "    # Iterate over both True and False values for use_bias\n",
        "    for use_bias_setting in [True, False]:\n",
        "        # Iterate over each structure_encoder configuration\n",
        "        for structure_encoder in structure_encoders:\n",
        "            # Iterate over each lag value\n",
        "            for lags_input in lags_inputs:\n",
        "                # Iterate over each link option\n",
        "                for link_option in link_options:\n",
        "                    # Iterate over each batch size option\n",
        "                    for batch_size_option in batch_size_options:\n",
        "                        # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                        ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                    optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                    batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                        # Start the timer for each individual model\n",
        "                        start_time_model = time.time()\n",
        "\n",
        "                        # Fit the model\n",
        "                        ddfm.fit()\n",
        "\n",
        "                        # Stop the timer for each individual model\n",
        "                        end_time_model = time.time()\n",
        "\n",
        "                        # Calculate the execution time for each individual model\n",
        "                        execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                        # Store the model, its configuration, final loss, and execution time\n",
        "                        final_loss = ddfm.loss_now\n",
        "                        # Store the model, its configuration, final loss, execution time, and r value\n",
        "                        model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                              execution_time_model, link_option, batch_size_option, r))\n",
        "\n",
        "\n",
        "    # Calculate the total execution time\n",
        "    end_time_total = time.time()\n",
        "    total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    model_info_dict_list = []\n",
        "    for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option , r_value in model_info:\n",
        "        model_info_dict = {\n",
        "            \"encoder_config\": encoder_config,\n",
        "            \"lag\": lag,\n",
        "            \"use_bias_setting\": use_bias_setting,\n",
        "            \"final_loss\": final_loss,\n",
        "            \"execution_time_model\": execution_time_model,\n",
        "            \"link_option\": link_option,\n",
        "            \"batch_size_option\": batch_size_option,\n",
        "            \"r_value\": r_value\n",
        "        }\n",
        "        model_info_dict_list.append(model_info_dict)\n",
        "\n",
        "    # Save the list of dictionaries to a JSON file\n",
        "    with open(f\"/content/drive/MyDrive/D2FM/model_info_r5.json\", \"w\") as json_file:\n",
        "        json.dump(model_info_dict_list, json_file)\n",
        "\n",
        "    print(f\"Results for r={r} saved to model_info_r{r}.json\")\n",
        "    print(f\"Total Execution Time for all models for r={r}: {total_execution_time:.2f} seconds\")\n",
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option, r_value in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, r_value)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option , r_value= min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}, r {r_value}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5yMbVvsISKQ",
        "outputId": "4f56e7cf-76a2-4edf-8708-e73657db880b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5727474687587538 - delta: 0.0003824681869214333 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5659209500689458 - delta: 0.00011515628523569736 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5807477376922936 - delta: 0.0003383810926425854 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5682630322570074 - delta: 0.0003281696843942158 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5603617106899877 - delta: 0.00021160238970096345 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5602679102676897 - delta: 0.00040538094503768444 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5613550311237003 - delta: 0.000495236971685609 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 5ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5604106964491578 - delta: 0.00039988147351034225 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5748363037193799 - delta: 0.00044243583831801844 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5633115030729804 - delta: 0.0003739337110734683 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5760568408583611 - delta: 2.0743600421396878e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5630170055685354 - delta: 0.00048102686555135624 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.560155765006405 - delta: 0.00030317792619578864 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.560605760588918 - delta: 0.00033827988636746893 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5606434583573945 - delta: 0.00033329797507266023 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5602847627864812 - delta: 0.00016615305889247536 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5656591530192152 - delta: 7.884621581669361e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6006097626881776 - delta: 0.0001998166137252579 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5836104150125303 - delta: 0.0004549125600370732 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 5ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5607966007049255 - delta: 0.00045489072749222494 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5610324179327317 - delta: 0.00048649071518807815 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5792845952789958 - delta: 0.0004969421421424325 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5794780788118945 - delta: 0.0004317691820632127 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.563463827491213 - delta: 0.00048227368142619013 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6001371169322419 - delta: 0.0003625814271371014 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 5ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5818034412746148 - delta: 0.00043358622297076844 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5603214383620926 - delta: 0.0004685411940881005 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 5ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5604270995887913 - delta: 0.00029711660473761534 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5781525977271063 - delta: 0.0004896037257688955 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5775292924842137 - delta: 0.0003939507281195667 < 0.0005\n",
            "Results for r=7 saved to model_info_r7.json\n",
            "Total Execution Time for all models for r=7: 4728.65 seconds\n",
            "Minimum Loss: 0.560155765006405 with encoder (168, 84, 56, 7), lag 1, use_bias True, link tanh, batch_size 72, r 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados asociados a la minima Loss:\n",
        "\n",
        "* Minimum Loss: 0.6215284234293389 with encoder (120, 60, 40, 5), lag 1, use_bias False, link tanh, batch_size 72, r 5\n",
        "\n",
        "\n",
        "* Minimum Loss: 0.589956869321811 with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 100, r 6\n",
        "\n",
        "* Minimum Loss: 0.560155765006405 with encoder (168, 84, 56, 7), lag 1, use_bias True, link tanh, batch_size 72, r 7"
      ],
      "metadata": {
        "id": "6E4GOXLIutRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################## no borrar ##################\n",
        "# comparacion con los 3 factores\n",
        "######################################################\n",
        "import time\n",
        "\n",
        "# Outer loop for r values\n",
        "for r in [5, 6, 7]:\n",
        "    start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "    structure_encoders = [(r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r)]\n",
        "\n",
        "    lags_inputs = list(range(2))\n",
        "    link_options = ['tanh', 'relu']\n",
        "    batch_size_options = [72, 100]\n",
        "\n",
        "    model_info = []\n",
        "\n",
        "    # Iterate over both True and False values for use_bias\n",
        "    for use_bias_setting in [True, False]:\n",
        "        # Iterate over each structure_encoder configuration\n",
        "        for structure_encoder in structure_encoders:\n",
        "            # Iterate over each lag value\n",
        "            for lags_input in lags_inputs:\n",
        "                # Iterate over each link option\n",
        "                for link_option in link_options:\n",
        "                    # Iterate over each batch size option\n",
        "                    for batch_size_option in batch_size_options:\n",
        "                        # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                        ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                    optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                    batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                        # Start the timer for each individual model\n",
        "                        start_time_model = time.time()\n",
        "\n",
        "                        # Fit the model\n",
        "                        ddfm.fit()\n",
        "\n",
        "                        # Stop the timer for each individual model\n",
        "                        end_time_model = time.time()\n",
        "\n",
        "                        # Calculate the execution time for each individual model\n",
        "                        execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                        # Store the model, its configuration, final loss, and execution time\n",
        "                        final_loss = ddfm.loss_now\n",
        "                        # Store the model, its configuration, final loss, execution time, and r value\n",
        "                        model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                              execution_time_model, link_option, batch_size_option, r))\n",
        "\n",
        "\n",
        "    # Calculate the total execution time\n",
        "    end_time_total = time.time()\n",
        "    total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "    # Print the information for each model\n",
        "    for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option , r_value in model_info:\n",
        "        print(f\"Model with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}, r {r_value} - Final Loss: {final_loss}\")\n",
        "        print(f\"Execution Time for this model: {execution_time_model:.2f} seconds\")\n",
        "\n",
        "    print(f\"Total Execution Time for all models: {total_execution_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w9hrVBaH8Xs",
        "outputId": "cf98f7e8-6c7e-4d2a-e177-18116a8e7334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6339407159860809 - delta: 9.398676472979998e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6289796917417518 - delta: 0.00048051832003069724 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6399867368474991 - delta: 1.1401713881717915e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6295021626708045 - delta: 0.00017737622706977927 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6222877991209359 - delta: 0.0003756796052888426 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6221596668514555 - delta: 0.0004398785557543964 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6220410241880099 - delta: 0.0003397607134010878 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6223163588127892 - delta: 0.0002612673097361933 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6257538650077075 - delta: 0.00037116488673643526 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.6373804768641256 - delta: 0.0004572394971258931 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6264584028056687 - delta: 0.0004830212451146382 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6219131862929351 - delta: 0.0004795793094818956 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6220487997012756 - delta: 0.000475009406789546 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6229072731514023 - delta: 4.8185391150212555e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.623795711994424 - delta: 0.0003693755489433139 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6336235222961986 - delta: 0.00014189768836032332 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6297929815273974 - delta: 0.0001333685480955968 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6616953039478541 - delta: 0.0004268285069982719 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6506668166166493 - delta: 0.00037861576275830054 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.622524707979739 - delta: 0.00034636773890661557 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6222937947910278 - delta: 0.0004324655327389779 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6461187841725745 - delta: 0.00018194053263968064 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6454123568842233 - delta: 0.00014623432989192896 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6347681282356762 - delta: 0.00014986485904064565 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6257184693187666 - delta: 0.00035672389282488513 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6596060167063007 - delta: 0.0002933483594288145 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6514904470178056 - delta: 0.0003852443719656454 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6220452386307327 - delta: 0.00048251737135134373 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6219493949718142 - delta: 0.0004962848533499032 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6447216728378337 - delta: 0.00047854175054285245 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6444813810732626 - delta: 0.00044238331292969287 < 0.0005\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias True, link tanh, batch_size 72, r 5 - Final Loss: 0.6339407159860809\n",
            "Execution Time for this model: 60.29 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias True, link tanh, batch_size 100, r 5 - Final Loss: 0.6289796917417518\n",
            "Execution Time for this model: 100.36 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias True, link relu, batch_size 72, r 5 - Final Loss: 0.6399867368474991\n",
            "Execution Time for this model: 81.70 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias True, link relu, batch_size 100, r 5 - Final Loss: 0.6295021626708045\n",
            "Execution Time for this model: 55.20 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias True, link tanh, batch_size 72, r 5 - Final Loss: 0.6222877991209359\n",
            "Execution Time for this model: 101.19 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias True, link tanh, batch_size 100, r 5 - Final Loss: 0.6221596668514555\n",
            "Execution Time for this model: 101.95 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias True, link relu, batch_size 72, r 5 - Final Loss: 0.6220410241880099\n",
            "Execution Time for this model: 67.54 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias True, link relu, batch_size 100, r 5 - Final Loss: 0.6223163588127892\n",
            "Execution Time for this model: 56.25 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias True, link tanh, batch_size 72, r 5 - Final Loss: 0.6361260814462544\n",
            "Execution Time for this model: 158.26 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias True, link tanh, batch_size 100, r 5 - Final Loss: 0.6257538650077075\n",
            "Execution Time for this model: 92.29 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias True, link relu, batch_size 72, r 5 - Final Loss: 0.6373804768641256\n",
            "Execution Time for this model: 141.47 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias True, link relu, batch_size 100, r 5 - Final Loss: 0.6264584028056687\n",
            "Execution Time for this model: 91.98 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias True, link tanh, batch_size 72, r 5 - Final Loss: 0.6219131862929351\n",
            "Execution Time for this model: 91.03 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias True, link tanh, batch_size 100, r 5 - Final Loss: 0.6220487997012756\n",
            "Execution Time for this model: 83.11 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias True, link relu, batch_size 72, r 5 - Final Loss: 0.6229072731514023\n",
            "Execution Time for this model: 65.73 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias True, link relu, batch_size 100, r 5 - Final Loss: 0.623795711994424\n",
            "Execution Time for this model: 58.43 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias False, link tanh, batch_size 72, r 5 - Final Loss: 0.6336235222961986\n",
            "Execution Time for this model: 73.00 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias False, link tanh, batch_size 100, r 5 - Final Loss: 0.6297929815273974\n",
            "Execution Time for this model: 77.09 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias False, link relu, batch_size 72, r 5 - Final Loss: 0.6616953039478541\n",
            "Execution Time for this model: 93.48 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 0, use_bias False, link relu, batch_size 100, r 5 - Final Loss: 0.6506668166166493\n",
            "Execution Time for this model: 87.49 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias False, link tanh, batch_size 72, r 5 - Final Loss: 0.622524707979739\n",
            "Execution Time for this model: 89.51 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias False, link tanh, batch_size 100, r 5 - Final Loss: 0.6222937947910278\n",
            "Execution Time for this model: 102.81 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias False, link relu, batch_size 72, r 5 - Final Loss: 0.6461187841725745\n",
            "Execution Time for this model: 99.68 seconds\n",
            "Model with encoder (60, 30, 20, 5), lag 1, use_bias False, link relu, batch_size 100, r 5 - Final Loss: 0.6454123568842233\n",
            "Execution Time for this model: 103.43 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias False, link tanh, batch_size 72, r 5 - Final Loss: 0.6347681282356762\n",
            "Execution Time for this model: 117.82 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias False, link tanh, batch_size 100, r 5 - Final Loss: 0.6257184693187666\n",
            "Execution Time for this model: 93.21 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias False, link relu, batch_size 72, r 5 - Final Loss: 0.6596060167063007\n",
            "Execution Time for this model: 131.86 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 0, use_bias False, link relu, batch_size 100, r 5 - Final Loss: 0.6514904470178056\n",
            "Execution Time for this model: 56.17 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias False, link tanh, batch_size 72, r 5 - Final Loss: 0.6220452386307327\n",
            "Execution Time for this model: 90.71 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias False, link tanh, batch_size 100, r 5 - Final Loss: 0.6219493949718142\n",
            "Execution Time for this model: 84.18 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias False, link relu, batch_size 72, r 5 - Final Loss: 0.6447216728378337\n",
            "Execution Time for this model: 100.46 seconds\n",
            "Model with encoder (120, 60, 40, 5), lag 1, use_bias False, link relu, batch_size 100, r 5 - Final Loss: 0.6444813810732626\n",
            "Execution Time for this model: 99.29 seconds\n",
            "Total Execution Time for all models: 2907.51 seconds\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6031715945396632 - delta: 0.0002651359369669384 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5964216318199527 - delta: 0.00013933040088355648 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6115543414039752 - delta: 3.342402856891561e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5957904269513004 - delta: 0.0003148542509782703 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5902858013191816 - delta: 0.0003550569714135523 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5903233289108948 - delta: 0.000493197660159105 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.590353698817376 - delta: 0.00035967045556562676 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5902453175299353 - delta: 2.0250175114330423e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6015538351204985 - delta: 1.094253531779388e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5933366071054291 - delta: 0.0002514260425235672 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6055723838251575 - delta: 0.0002284400837508938 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5936041650380768 - delta: 0.00042709245590608633 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5901778586686498 - delta: 0.00023592740357910414 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5901780443676836 - delta: 0.00037480690984037626 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5900839189627851 - delta: 0.00023773753381113073 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5923114299184027 - delta: 0.00022539218177276805 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5958430731620376 - delta: 0.0002573136705686561 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6314201828211348 - delta: 0.00017137899849214595 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6144637187209495 - delta: 0.00032506438105115067 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5901786883877496 - delta: 0.00039524814152110235 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5907595096282741 - delta: 0.00045283397698123087 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6078705014735811 - delta: 0.00024252802858223529 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6082555814532185 - delta: 0.0004458541919500399 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6041178432388231 - delta: 0.00022162424998512564 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5950371138968429 - delta: 5.965033422925475e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6304382287079093 - delta: 0.00019230126947289625 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6136779418703722 - delta: 0.00035075363394927186 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5903448752146511 - delta: 0.0004406645984055576 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5899110872683166 - delta: 0.0002201360519522426 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.608915481451977 - delta: 0.00024010325306947412 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6076517444080927 - delta: 0.00028525542377575336 < 0.0005\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 72, r 6 - Final Loss: 0.6031715945396632\n",
            "Execution Time for this model: 88.70 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 100, r 6 - Final Loss: 0.5964216318199527\n",
            "Execution Time for this model: 91.53 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 72, r 6 - Final Loss: 0.6115543414039752\n",
            "Execution Time for this model: 65.26 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 100, r 6 - Final Loss: 0.5957904269513004\n",
            "Execution Time for this model: 70.70 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 72, r 6 - Final Loss: 0.5902858013191816\n",
            "Execution Time for this model: 96.49 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 100, r 6 - Final Loss: 0.5903233289108948\n",
            "Execution Time for this model: 97.84 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 72, r 6 - Final Loss: 0.590353698817376\n",
            "Execution Time for this model: 83.75 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 100, r 6 - Final Loss: 0.5902453175299353\n",
            "Execution Time for this model: 85.01 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 72, r 6 - Final Loss: 0.6015538351204985\n",
            "Execution Time for this model: 122.82 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 100, r 6 - Final Loss: 0.5933366071054291\n",
            "Execution Time for this model: 109.28 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 72, r 6 - Final Loss: 0.6055723838251575\n",
            "Execution Time for this model: 97.17 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 100, r 6 - Final Loss: 0.5936041650380768\n",
            "Execution Time for this model: 83.91 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 72, r 6 - Final Loss: 0.5901778586686498\n",
            "Execution Time for this model: 104.97 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 100, r 6 - Final Loss: 0.5901780443676836\n",
            "Execution Time for this model: 87.30 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 72, r 6 - Final Loss: 0.5900839189627851\n",
            "Execution Time for this model: 93.15 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 100, r 6 - Final Loss: 0.5923114299184027\n",
            "Execution Time for this model: 71.56 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 72, r 6 - Final Loss: 0.6042862031591865\n",
            "Execution Time for this model: 150.81 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 100, r 6 - Final Loss: 0.5958430731620376\n",
            "Execution Time for this model: 123.14 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 72, r 6 - Final Loss: 0.6314201828211348\n",
            "Execution Time for this model: 100.16 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 100, r 6 - Final Loss: 0.6144637187209495\n",
            "Execution Time for this model: 99.44 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 72, r 6 - Final Loss: 0.5901786883877496\n",
            "Execution Time for this model: 86.65 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 100, r 6 - Final Loss: 0.5907595096282741\n",
            "Execution Time for this model: 83.40 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 72, r 6 - Final Loss: 0.6078705014735811\n",
            "Execution Time for this model: 96.93 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 100, r 6 - Final Loss: 0.6082555814532185\n",
            "Execution Time for this model: 77.12 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 72, r 6 - Final Loss: 0.6041178432388231\n",
            "Execution Time for this model: 97.50 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 100, r 6 - Final Loss: 0.5950371138968429\n",
            "Execution Time for this model: 100.41 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 72, r 6 - Final Loss: 0.6304382287079093\n",
            "Execution Time for this model: 90.08 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 100, r 6 - Final Loss: 0.6136779418703722\n",
            "Execution Time for this model: 70.02 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 72, r 6 - Final Loss: 0.5903448752146511\n",
            "Execution Time for this model: 90.27 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 100, r 6 - Final Loss: 0.5899110872683166\n",
            "Execution Time for this model: 91.49 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 72, r 6 - Final Loss: 0.608915481451977\n",
            "Execution Time for this model: 76.72 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 100, r 6 - Final Loss: 0.6076517444080927\n",
            "Execution Time for this model: 99.86 seconds\n",
            "Total Execution Time for all models: 2984.00 seconds\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5747983920525954 - delta: 0.0004617152249124705 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5665828846184694 - delta: 0.00012722783684231592 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5779507052947164 - delta: 0.0004825790101441016 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5656481322115874 - delta: 5.628124947102197e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5605270073633709 - delta: 0.00046995461816839643 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5601523673965111 - delta: 0.00042009170055574453 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5611992418960707 - delta: 0.0003414692005331668 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5605584298157035 - delta: 0.0003883235005904344 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5637702924815303 - delta: 0.0003292463847197253 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5754606253325756 - delta: 0.00041027162448869137 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5644803738928763 - delta: 0.0004788063077402655 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.560476366434764 - delta: 0.0004236499267606825 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5604567045376723 - delta: 0.0002992266496155356 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5614390709655742 - delta: 0.00044660735575718924 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.560814919814407 - delta: 0.00047000523594978137 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5737457986661583 - delta: 0.00015743119077566299 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.565918953681962 - delta: 0.00042475069298007924 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6024940403780616 - delta: 3.344391775042411e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5840077283636865 - delta: 0.0003120844044455134 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5602091077343921 - delta: 0.00029475018431871506 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.560107045756187 - delta: 0.0003200029197287608 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5803972142052052 - delta: 0.0004807626259544481 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5788180558401589 - delta: 0.0004973851601900365 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5732653293755379 - delta: 0.00011984977855362556 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5632204714846718 - delta: 0.00032802788248029543 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.598220331031246 - delta: 8.678063525752392e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5819789206714092 - delta: 0.0004648195050011908 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.560203054559434 - delta: 0.00035087581924067937 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5600313739508542 - delta: 0.0004876385518801979 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5792253735970239 - delta: 0.00045661322997582357 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.5778817271012269 - delta: 0.00026550160347730975 < 0.0005\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias True, link tanh, batch_size 72, r 7 - Final Loss: 0.5747983920525954\n",
            "Execution Time for this model: 98.76 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias True, link tanh, batch_size 100, r 7 - Final Loss: 0.5665828846184694\n",
            "Execution Time for this model: 86.42 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias True, link relu, batch_size 72, r 7 - Final Loss: 0.5779507052947164\n",
            "Execution Time for this model: 129.11 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias True, link relu, batch_size 100, r 7 - Final Loss: 0.5656481322115874\n",
            "Execution Time for this model: 89.27 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias True, link tanh, batch_size 72, r 7 - Final Loss: 0.5605270073633709\n",
            "Execution Time for this model: 90.09 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias True, link tanh, batch_size 100, r 7 - Final Loss: 0.5601523673965111\n",
            "Execution Time for this model: 109.74 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias True, link relu, batch_size 72, r 7 - Final Loss: 0.5611992418960707\n",
            "Execution Time for this model: 56.65 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias True, link relu, batch_size 100, r 7 - Final Loss: 0.5605584298157035\n",
            "Execution Time for this model: 80.34 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias True, link tanh, batch_size 72, r 7 - Final Loss: 0.5716873205596793\n",
            "Execution Time for this model: 167.22 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias True, link tanh, batch_size 100, r 7 - Final Loss: 0.5637702924815303\n",
            "Execution Time for this model: 96.16 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias True, link relu, batch_size 72, r 7 - Final Loss: 0.5754606253325756\n",
            "Execution Time for this model: 106.48 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias True, link relu, batch_size 100, r 7 - Final Loss: 0.5644803738928763\n",
            "Execution Time for this model: 76.10 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias True, link tanh, batch_size 72, r 7 - Final Loss: 0.560476366434764\n",
            "Execution Time for this model: 92.17 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias True, link tanh, batch_size 100, r 7 - Final Loss: 0.5604567045376723\n",
            "Execution Time for this model: 91.82 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias True, link relu, batch_size 72, r 7 - Final Loss: 0.5614390709655742\n",
            "Execution Time for this model: 65.03 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias True, link relu, batch_size 100, r 7 - Final Loss: 0.560814919814407\n",
            "Execution Time for this model: 73.30 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias False, link tanh, batch_size 72, r 7 - Final Loss: 0.5737457986661583\n",
            "Execution Time for this model: 85.52 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias False, link tanh, batch_size 100, r 7 - Final Loss: 0.565918953681962\n",
            "Execution Time for this model: 98.22 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias False, link relu, batch_size 72, r 7 - Final Loss: 0.6024940403780616\n",
            "Execution Time for this model: 56.45 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 0, use_bias False, link relu, batch_size 100, r 7 - Final Loss: 0.5840077283636865\n",
            "Execution Time for this model: 97.55 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias False, link tanh, batch_size 72, r 7 - Final Loss: 0.5602091077343921\n",
            "Execution Time for this model: 103.03 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias False, link tanh, batch_size 100, r 7 - Final Loss: 0.560107045756187\n",
            "Execution Time for this model: 112.11 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias False, link relu, batch_size 72, r 7 - Final Loss: 0.5803972142052052\n",
            "Execution Time for this model: 105.83 seconds\n",
            "Model with encoder (84, 42, 28, 7), lag 1, use_bias False, link relu, batch_size 100, r 7 - Final Loss: 0.5788180558401589\n",
            "Execution Time for this model: 113.66 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias False, link tanh, batch_size 72, r 7 - Final Loss: 0.5732653293755379\n",
            "Execution Time for this model: 52.09 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias False, link tanh, batch_size 100, r 7 - Final Loss: 0.5632204714846718\n",
            "Execution Time for this model: 110.18 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias False, link relu, batch_size 72, r 7 - Final Loss: 0.598220331031246\n",
            "Execution Time for this model: 90.88 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 0, use_bias False, link relu, batch_size 100, r 7 - Final Loss: 0.5819789206714092\n",
            "Execution Time for this model: 104.40 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias False, link tanh, batch_size 72, r 7 - Final Loss: 0.560203054559434\n",
            "Execution Time for this model: 102.22 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias False, link tanh, batch_size 100, r 7 - Final Loss: 0.5600313739508542\n",
            "Execution Time for this model: 100.01 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias False, link relu, batch_size 72, r 7 - Final Loss: 0.5792253735970239\n",
            "Execution Time for this model: 103.26 seconds\n",
            "Model with encoder (168, 84, 56, 7), lag 1, use_bias False, link relu, batch_size 100, r 7 - Final Loss: 0.5778817271012269\n",
            "Execution Time for this model: 129.58 seconds\n",
            "Total Execution Time for all models: 3074.18 seconds\n",
            "Minimum Loss: 0.5600313739508542 with encoder (168, 84, 56, 7), lag 1, use_bias False, link tanh, batch_size 100, r 7\n",
            "Model information saved to model_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option, r_value in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, r_value)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option , r_value= min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}, r {r_value}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TggxMUyNG43",
        "outputId": "4977c3b7-527a-4142-b307-f5a33331b894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Loss: 0.5600313739508542 with encoder (168, 84, 56, 7), lag 1, use_bias False, link tanh, batch_size 100, r 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Create a list to store dictionaries representing each model\n",
        "model_info_dict_list = []\n",
        "\n",
        "# Iterate over each model's information and convert it into a dictionary\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option, r_value in model_info:\n",
        "    model_info_dict = {\n",
        "        \"encoder_config\": encoder_config,\n",
        "        \"lag\": lag,\n",
        "        \"use_bias_setting\": use_bias_setting,\n",
        "        \"final_loss\": final_loss,\n",
        "        \"execution_time_model\": execution_time_model,\n",
        "        \"link_option\": link_option,\n",
        "        \"batch_size_option\": batch_size_option,\n",
        "        \"r_value\": r_value\n",
        "    }\n",
        "    model_info_dict_list.append(model_info_dict)\n",
        "\n",
        "# Save the list of dictionaries to a JSON file\n",
        "with open(\"/content/drive/MyDrive/D2FM/model_info.json\", \"w\") as json_file:\n",
        "    json.dump(model_info_dict_list, json_file)\n",
        "\n",
        "print(\"Model information saved to model_info.json\")"
      ],
      "metadata": {
        "id": "pSDsOXuSwgAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93b2f3b6-1ce8-4d10-f3c5-e7322753d8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model information saved to model_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store minimum loss and associated configuration for each r value\n",
        "min_loss_configs = {}\n",
        "\n",
        "# Iterate over unique r values\n",
        "for r_value in set(r for _, _, _, _, _, _, _, _, r in model_info):\n",
        "    # Initialize variables to store the minimum loss and associated configuration for current r value\n",
        "    min_loss = float('inf')\n",
        "    min_loss_config = None\n",
        "\n",
        "    # Iterate over the stored models to find the one with the minimum loss for current r value\n",
        "    for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option, r in model_info:\n",
        "        # Check if current model corresponds to current r value\n",
        "        if r == r_value:\n",
        "            # Update min_loss and min_loss_config if current loss is smaller\n",
        "            if final_loss < min_loss:\n",
        "                min_loss = final_loss\n",
        "                min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, r)\n",
        "\n",
        "    # Store the minimum loss and associated configuration for current r value in the dictionary\n",
        "    min_loss_configs[r_value] = (min_loss, min_loss_config)\n",
        "\n",
        "# Print the configurations with the minimum loss for each r value\n",
        "for r_value in sorted(min_loss_configs.keys()):\n",
        "    min_loss, min_loss_config = min_loss_configs[r_value]\n",
        "    if min_loss_config is not None:\n",
        "        encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option, _ = min_loss_config\n",
        "        print(f\"Minimum Loss for r={r_value}: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}\")\n",
        "    else:\n",
        "        print(f\"No models found or no valid loss values for r={r_value}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqfOa0-moONy",
        "outputId": "7b0e05f4-919f-4863-a28a-d42fd7420aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Loss for r=7: 0.5600313739508542 with encoder (168, 84, 56, 7), lag 1, use_bias False, link tanh, batch_size 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time_total = time.time()  # Record the start time for the entire process\n",
        "r = 6\n",
        "structure_encoders = [(r * 6, r * 4, r * 2, r),\n",
        "                      (r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r), (r * 48, r * 24, r * 16, r)]\n",
        "\n",
        "lags_inputs = list(range(2))\n",
        "link_options = ['tanh', 'relu']\n",
        "batch_size_options = [24, 100]\n",
        "\n",
        "model_info = []\n",
        "\n",
        "# Iterate over both True and False values for use_bias\n",
        "for use_bias_setting in [True, False]:\n",
        "    # Iterate over each structure_encoder configuration\n",
        "    for structure_encoder in structure_encoders:\n",
        "        # Iterate over each lag value\n",
        "        for lags_input in lags_inputs:\n",
        "            # Iterate over each link option\n",
        "            for link_option in link_options:\n",
        "                # Iterate over each batch size option\n",
        "                for batch_size_option in batch_size_options:\n",
        "                    # Initialize the DDFM with the current configuration and use_bias setting\n",
        "                    ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                                optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link=link_option,\n",
        "                                batch_size=batch_size_option, epochs=100, max_iter=1000)\n",
        "\n",
        "                    # Start the timer for each individual model\n",
        "                    start_time_model = time.time()\n",
        "\n",
        "                    # Fit the model\n",
        "                    ddfm.fit()\n",
        "\n",
        "                    # Stop the timer for each individual model\n",
        "                    end_time_model = time.time()\n",
        "\n",
        "                    # Calculate the execution time for each individual model\n",
        "                    execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "                    # Store the model, its configuration, final loss, and execution time\n",
        "                    final_loss = ddfm.loss_now\n",
        "                    model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss,\n",
        "                                       execution_time_model, link_option, batch_size_option))\n",
        "\n",
        "\n",
        "# Calculate the total execution time\n",
        "end_time_total = time.time()\n",
        "total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "# Print the information for each model\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model, link_option, batch_size_option in model_info:\n",
        "    print(f\"Model with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option} - Final Loss: {final_loss}\")\n",
        "    print(f\"Execution Time for this model: {execution_time_model:.2f} seconds\")\n",
        "\n",
        "print(f\"Total Execution Time for all models: {total_execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyrKEnejHJmD",
        "outputId": "945740cb-c41b-40fc-d17d-6db131ad2626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6062719455379042 - delta: 0.0001527789508580558 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6042899097289977 - delta: 0.0003065221216681004 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6023843152966347 - delta: 3.685885736727355e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.598491798566092 - delta: 0.00015911664960780816 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6002428834341017 - delta: 0.00022240091485573735 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5910242783356396 - delta: 0.00034407843139337653 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5956880450291226 - delta: 0.0003653389912184356 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5914107026509534 - delta: 0.00017277716381314826 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6049391147003528 - delta: 4.43864744965334e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 9 iterations - new loss: 0.5967772255100906 - delta: 4.5609778047947735e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6029460508896672 - delta: 0.00044028349341887143 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5954505126447965 - delta: 7.832076467386196e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5964943410600487 - delta: 0.00018666113470886596 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5899213905144091 - delta: 0.0004646270748447459 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5949323333126293 - delta: 0.0001925431542131662 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5912581728510942 - delta: 0.0003789040012973617 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6003987766262829 - delta: 0.0001032932313131454 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.593671649027021 - delta: 0.00032187867059967496 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6040446630202766 - delta: 0.000215595811448479 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5942885950769177 - delta: 0.00038332508857920637 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5935188389604059 - delta: 0.0004870004825115737 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897715899469995 - delta: 0.00027790505958715824 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5969073764109344 - delta: 0.00023307122122015749 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5911522079270886 - delta: 0.00030634811276733097 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5981758686034958 - delta: 0.0004829542089740189 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5921660183819364 - delta: 0.0003355168475193326 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6070911675236714 - delta: 0.000300076734324492 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5960229812381836 - delta: 0.00020664514928323208 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5927569656701296 - delta: 0.00046259972431036966 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.58967100937883 - delta: 0.0004644089524920016 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5979895264273086 - delta: 0.00010401898039127872 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5907699456273472 - delta: 8.132066577197144e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6062548934065508 - delta: 8.275697284395381e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.602727405145712 - delta: 6.244500355134345e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6308891588908048 - delta: 0.00017148620474028797 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6181232528313846 - delta: 0.00027063700397867297 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5984879606530491 - delta: 0.00030098281833463263 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5921400058562918 - delta: 0.00039660292308030015 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6219090376198234 - delta: 0.00038408998008114526 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.6090994685171214 - delta: 0.0004812029318313043 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6053986780251778 - delta: 0.00039232346802667295 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.596581662720132 - delta: 0.0004589913492979657 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6295127707356876 - delta: 0.00044012391129523746 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.614537528427105 - delta: 0.0004365875473530747 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5960004367749229 - delta: 0.0004874662874916214 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5906107643512373 - delta: 0.00042970398916054923 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6183733277886364 - delta: 0.00013178379302344008 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.6082655107916747 - delta: 0.0003275360061450031 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5984035437947037 - delta: 0.00018014127683493717 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5951782881046199 - delta: 3.425342455849549e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.6327597742232652 - delta: 0.00016205049165547844 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5936419537010068 - delta: 0.00046950706780443305 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5900544032161384 - delta: 0.0004973231190222807 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.615847745675623 - delta: 3.4310620136067316e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6076975918395733 - delta: 0.0004126154247404874 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5968888891013979 - delta: 0.00037840389436479915 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.592358968378782 - delta: 0.0003966710950347309 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6291393594869089 - delta: 0.00019733083599424208 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 8 iterations - new loss: 0.6097625812669517 - delta: 0.00024543045270425466 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 3 iterations - new loss: 0.5930709867386083 - delta: 0.00045146161667056667 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897144527534139 - delta: 0.00044150957760852127 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.6218347326856096 - delta: 0.0002936154484380311 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6091593557346485 - delta: 0.0003080609697254865 < 0.0005\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6062719455379042\n",
            "Execution Time for this model: 106.63 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.6042899097289977\n",
            "Execution Time for this model: 77.59 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6023843152966347\n",
            "Execution Time for this model: 64.69 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.598491798566092\n",
            "Execution Time for this model: 48.78 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6002428834341017\n",
            "Execution Time for this model: 89.64 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5910242783356396\n",
            "Execution Time for this model: 124.51 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5956880450291226\n",
            "Execution Time for this model: 77.37 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5914107026509534\n",
            "Execution Time for this model: 79.49 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6049391147003528\n",
            "Execution Time for this model: 100.84 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5967772255100906\n",
            "Execution Time for this model: 130.39 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6029460508896672\n",
            "Execution Time for this model: 78.77 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5954505126447965\n",
            "Execution Time for this model: 90.23 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5964943410600487\n",
            "Execution Time for this model: 87.74 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5899213905144091\n",
            "Execution Time for this model: 103.29 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5949323333126293\n",
            "Execution Time for this model: 77.84 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5912581728510942\n",
            "Execution Time for this model: 78.39 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.6003987766262829\n",
            "Execution Time for this model: 79.24 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.593671649027021\n",
            "Execution Time for this model: 103.35 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6040446630202766\n",
            "Execution Time for this model: 65.67 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5942885950769177\n",
            "Execution Time for this model: 68.47 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5935188389604059\n",
            "Execution Time for this model: 73.87 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5897715899469995\n",
            "Execution Time for this model: 98.33 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5969073764109344\n",
            "Execution Time for this model: 59.52 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5911522079270886\n",
            "Execution Time for this model: 92.67 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5981758686034958\n",
            "Execution Time for this model: 89.20 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link tanh, batch_size 100 - Final Loss: 0.5921660183819364\n",
            "Execution Time for this model: 120.82 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link relu, batch_size 24 - Final Loss: 0.6070911675236714\n",
            "Execution Time for this model: 89.55 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True, link relu, batch_size 100 - Final Loss: 0.5960229812381836\n",
            "Execution Time for this model: 51.70 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 24 - Final Loss: 0.5927569656701296\n",
            "Execution Time for this model: 93.99 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 100 - Final Loss: 0.58967100937883\n",
            "Execution Time for this model: 92.57 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link relu, batch_size 24 - Final Loss: 0.5979895264273086\n",
            "Execution Time for this model: 78.69 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True, link relu, batch_size 100 - Final Loss: 0.5907699456273472\n",
            "Execution Time for this model: 78.98 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.6062548934065508\n",
            "Execution Time for this model: 75.34 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.602727405145712\n",
            "Execution Time for this model: 86.65 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6308891588908048\n",
            "Execution Time for this model: 120.70 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6181232528313846\n",
            "Execution Time for this model: 100.17 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5984879606530491\n",
            "Execution Time for this model: 86.07 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5921400058562918\n",
            "Execution Time for this model: 86.51 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6219090376198234\n",
            "Execution Time for this model: 60.16 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6090994685171214\n",
            "Execution Time for this model: 103.41 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.6053986780251778\n",
            "Execution Time for this model: 61.55 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.596581662720132\n",
            "Execution Time for this model: 110.28 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6295127707356876\n",
            "Execution Time for this model: 60.67 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.614537528427105\n",
            "Execution Time for this model: 111.85 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5960004367749229\n",
            "Execution Time for this model: 75.65 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5906107643512373\n",
            "Execution Time for this model: 81.79 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6183733277886364\n",
            "Execution Time for this model: 72.95 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6082655107916747\n",
            "Execution Time for this model: 76.76 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5984035437947037\n",
            "Execution Time for this model: 77.20 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5951782881046199\n",
            "Execution Time for this model: 107.97 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6327597742232652\n",
            "Execution Time for this model: 77.64 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6111559978328007\n",
            "Execution Time for this model: 133.03 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5936419537010068\n",
            "Execution Time for this model: 74.19 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5900544032161384\n",
            "Execution Time for this model: 92.08 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.615847745675623\n",
            "Execution Time for this model: 94.08 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6076975918395733\n",
            "Execution Time for this model: 95.74 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5968888891013979\n",
            "Execution Time for this model: 91.24 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link tanh, batch_size 100 - Final Loss: 0.592358968378782\n",
            "Execution Time for this model: 103.44 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link relu, batch_size 24 - Final Loss: 0.6291393594869089\n",
            "Execution Time for this model: 67.05 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False, link relu, batch_size 100 - Final Loss: 0.6097625812669517\n",
            "Execution Time for this model: 132.32 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link tanh, batch_size 24 - Final Loss: 0.5930709867386083\n",
            "Execution Time for this model: 93.81 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link tanh, batch_size 100 - Final Loss: 0.5897144527534139\n",
            "Execution Time for this model: 104.88 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link relu, batch_size 24 - Final Loss: 0.6218347326856096\n",
            "Execution Time for this model: 76.57 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False, link relu, batch_size 100 - Final Loss: 0.6091593557346485\n",
            "Execution Time for this model: 77.63 seconds\n",
            "Total Execution Time for all models: 5623.29 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store the minimum loss and associated configuration.\n",
        "min_loss = float('inf')\n",
        "min_loss_config = None\n",
        "\n",
        "# Iterate over the stored models to find the one with the minimum loss.\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time, link_option, batch_size_option in model_info:\n",
        "    # Update min_loss and min_loss_config if current loss is smaller\n",
        "    if final_loss < min_loss:\n",
        "        min_loss = final_loss\n",
        "        min_loss_config = (encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option)\n",
        "\n",
        "# Print the configuration with the minimum loss.\n",
        "if min_loss_config is not None:\n",
        "    encoder_config, lag, use_bias_setting, execution_time, link_option, batch_size_option = min_loss_config\n",
        "    print(f\"Minimum Loss: {min_loss} with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting}, link {link_option}, batch_size {batch_size_option}\")\n",
        "else:\n",
        "    print(\"No models found or no valid loss values.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1zf-3RHeHj",
        "outputId": "d5889c7a-4940-456d-9a76-caf87a2f8491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum Loss: 0.58967100937883 with encoder (288, 144, 96, 6), lag 1, use_bias True, link tanh, batch_size 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para menos combinaciones de hiperparámetros, queda como ejemplo:"
      ],
      "metadata": {
        "id": "8TrKdbK9YDgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time_total = time.time()  # Record the start time for the entire process\n",
        "\n",
        "# Your existing code\n",
        "r = 6\n",
        "structure_encoders = [(r * 6, r * 4, r * 2, r),\n",
        "                      (r * 12, r * 6, r * 4, r), (r * 24, r * 12, r * 8, r), (r * 48, r * 24, r * 16, r),]\n",
        "\n",
        "lags_inputs = list(range(2))\n",
        "\n",
        "model_info = []\n",
        "\n",
        "# Iterate over both True and False values for use_bias\n",
        "for use_bias_setting in [True, False]:\n",
        "    # Iterate over each structure_encoder configuration\n",
        "    for structure_encoder in structure_encoders:\n",
        "        # Iterate over each lag value\n",
        "        for lags_input in lags_inputs:\n",
        "            # Initialize the DDFM with the current configuration and use_bias setting\n",
        "            ddfm = DDFM(df_train_idx, lags_input=lags_input, structure_encoder=structure_encoder,\n",
        "                        optimizer='Adam', factor_oder=1, use_bias=use_bias_setting, link='tanh',\n",
        "                        epochs=100, max_iter=1000)\n",
        "\n",
        "            # Start the timer for each individual model\n",
        "            start_time_model = time.time()\n",
        "\n",
        "            # Fit the model\n",
        "            ddfm.fit()\n",
        "\n",
        "            # Stop the timer for each individual model\n",
        "            end_time_model = time.time()\n",
        "\n",
        "            # Calculate the execution time for each individual model\n",
        "            execution_time_model = end_time_model - start_time_model\n",
        "\n",
        "            # Store the model, its configuration, final loss, and execution time\n",
        "            final_loss = ddfm.loss_now\n",
        "            model_info.append((ddfm, structure_encoder, lags_input, use_bias_setting, final_loss, execution_time_model))\n",
        "\n",
        "# Calculate the total execution time\n",
        "end_time_total = time.time()\n",
        "total_execution_time = end_time_total - start_time_total\n",
        "\n",
        "# Print the information for each model\n",
        "for model, encoder_config, lag, use_bias_setting, final_loss, execution_time_model in model_info:\n",
        "    print(f\"Model with encoder {encoder_config}, lag {lag}, use_bias {use_bias_setting} - Final Loss: {final_loss}\")\n",
        "    print(f\"Execution Time for this model: {execution_time_model:.2f} seconds\")\n",
        "\n",
        "print(f\"Total Execution Time for all models: {total_execution_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCmpQtbS8wLt",
        "outputId": "0f97f50d-9ae9-4bd9-bfd7-e073b05c607d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.6000213739457754 - delta: 2.948570757007814e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 2 iterations - new loss: 0.5981605811324008 - delta: 0.00041992750411991146 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6002641262117545 - delta: 0.0003938048813976006 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5901988115578353 - delta: 0.00020263298102667096 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5949236383805527 - delta: 3.758155916742125e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5899904092776949 - delta: 0.0004966915807349267 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5920543169590102 - delta: 0.00045281531054624754 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5897366841627117 - delta: 2.1335932081187726e-05 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.6018477178373662 - delta: 0.0002652733344047113 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 7 iterations - new loss: 0.5918873194336332 - delta: 0.0004368461688436822 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 5 iterations - new loss: 0.5986105833380825 - delta: 0.00018390257867544776 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5900989855087982 - delta: 0.0004843253657673757 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 4 iterations - new loss: 0.5953852975705818 - delta: 0.0003933090511046878 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5907555267971585 - delta: 0.0004861394215376436 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "19/19 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5922850032041527 - delta: 0.0004212111463192132 < 0.0005\n",
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.5896470056018115 - delta: 0.00037551778942354077 < 0.0005\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias True - Final Loss: 0.6000213739457754\n",
            "Execution Time for this model: 95.43 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias True - Final Loss: 0.5981605811324008\n",
            "Execution Time for this model: 48.35 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias True - Final Loss: 0.6002641262117545\n",
            "Execution Time for this model: 70.12 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias True - Final Loss: 0.5901988115578353\n",
            "Execution Time for this model: 92.47 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias True - Final Loss: 0.5949236383805527\n",
            "Execution Time for this model: 105.76 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias True - Final Loss: 0.5899904092776949\n",
            "Execution Time for this model: 83.75 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias True - Final Loss: 0.5920543169590102\n",
            "Execution Time for this model: 118.98 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias True - Final Loss: 0.5897366841627117\n",
            "Execution Time for this model: 108.83 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 0, use_bias False - Final Loss: 0.6018477178373662\n",
            "Execution Time for this model: 68.66 seconds\n",
            "Model with encoder (36, 24, 12, 6), lag 1, use_bias False - Final Loss: 0.5918873194336332\n",
            "Execution Time for this model: 100.38 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 0, use_bias False - Final Loss: 0.5986105833380825\n",
            "Execution Time for this model: 76.38 seconds\n",
            "Model with encoder (72, 36, 24, 6), lag 1, use_bias False - Final Loss: 0.5900989855087982\n",
            "Execution Time for this model: 90.85 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 0, use_bias False - Final Loss: 0.5953852975705818\n",
            "Execution Time for this model: 70.70 seconds\n",
            "Model with encoder (144, 72, 48, 6), lag 1, use_bias False - Final Loss: 0.5907555267971585\n",
            "Execution Time for this model: 93.51 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 0, use_bias False - Final Loss: 0.5922850032041527\n",
            "Execution Time for this model: 107.06 seconds\n",
            "Model with encoder (288, 144, 96, 6), lag 1, use_bias False - Final Loss: 0.5896470056018115\n",
            "Execution Time for this model: 107.68 seconds\n",
            "Total Execution Time for all models: 1439.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddfm6 = DDFM(df_train_idx, structure_encoder=(288, 144, 96, 6), lags_input=1, factor_oder=1,\n",
        "                             use_bias=False, link='tanh', max_iter=1000)\n",
        "ddfm6.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBWbuZwx_6MV",
        "outputId": "9f89ed79-9f4e-4827-ee9c-11d5ca3bc0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Info - Note: Sorting data.\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "@Info: Convergence achieved in 6 iterations - new loss: 0.589643687159019 - delta: 0.00012052289014438879 < 0.0005\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}